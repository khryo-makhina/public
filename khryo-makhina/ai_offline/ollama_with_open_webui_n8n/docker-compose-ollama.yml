# This was generated by https://copilot.microsoft.com ... mostly

# Build the Docker image:
# docker-compose -f docker-compose-ollama.yml build

# Start the services
# docker-compose -f docker-compose-ollama.yml up -d

# To access the ollama CLI in Docker Desktop:
# docker exec -it ollama bash

# Pull the required model
# ollama pull llama3.2:3b
# Errors:----------------------------------------------------------------------
# pulling manifest
# pulling dde5aa3fc5ff...   0% ▕                                                                 ▏    0 B/2.0 GB
# Error: max retries exceeded: Get "https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/dd/dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20250410%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20250410T200336Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e60024db6fc3b0094e25e9a8924f0a1759e098d54fb82485632eec28a0148031": tls: failed to verify certificate: x509: certificate signed by unknown authority

# To stop the services
# docker-compose -f docker-compose-ollama.yml down

# To remove the containers and images
# docker-compose -f docker-compose-ollama.yml down --rmi all

# To remove the volumes
# docker-compose -f docker-compose-ollama.yml down -v

services:
  ollama:
    build:
      context: .
      dockerfile: ollama_Dockerfile # Use the specified Dockerfile for the Ollama service
    container_name: ollama # Name the container "ollama" for easier identification    
    ports:
      - "11434:11434" # Map port 11434 on the host to port 11434 in the container
    volumes:
      - ./ollama_data:/root/.ollama # Persist Ollama data in the local directory "./ollama_data"
    environment:
      - CUDA_VISIBLE_DEVICES=0 # Specify which GPU to use (0 for the first GPU)
      - NVIDIA_VISIBLE_DEVICES=all # Make all NVIDIA GPUs visible to the container
      - CUDA_MEMORY_FRACTION=0.9 # Limit GPU memory usage to 90% to prevent over-allocation
      - TZ=UTC # Ensure correct system time
      - GODEBUG=x509ignoreCN=0 # Disable TLS verification (temporary)
    deploy:
      resources:
        limits:
          memory: 16G # Limit the container's memory usage to 16GB
        reservations:
          devices:
            - driver: nvidia # Use the NVIDIA GPU driver
              count: 1 # Reserve one GPU for this container
              capabilities: [gpu] # Specify that the container requires GPU capabilities
    shm_size: 8gb # Set shared memory size to 8GB to support applications that require large shared memory (e.g., machine learning models or large datasets)
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434 || exit 1"] # Ensure the health check exits with a non-zero code on failure
      interval: 30s # Perform the health check every 30 seconds
      timeout: 10s # Timeout for the health check is 10 seconds
      retries: 5 # Retry the health check up to 5 times before marking the container as unhealthy
      start_period: 10s # Add a start period to allow the service to initialize before health checks begin

networks:
  default:
    name: ai_offline_network # Define a custom name for the network